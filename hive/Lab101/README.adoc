= Hive Workshop - Lab101
<venky@cloudera.com>
v0.1, 2019-08-16: draft
:page-layout: docs
:description: Hive Workshop Lab 101
:icons: font
:uri-fontawesome: https://fontawesome.com/v4.7.0/
:imagesdir: ./images
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc:
:toc-placement!:

[abstract]

toc::[]

=== Intro
In this lab, we will learn how to interact with Hive using HiveQL, do basic CRUD operations and work with External Tables.

* Sanity Check
** Login to Ambari and check if services are green.
+
[source]
----
username: admin
password: BadPass#1
----

* Working With HiveQL
** ssh into your cluster https://github.com/vsellappa/workshop/tree/master/connect[how-to]

** Change into the service `etl_user`.
+
[source,bash]
----
sudo su - etl_user
----
** `kinit` the user `etl_user`.
+
[source, bash]
----
kinit -Vkt /etc/security/keytabs/etl_user.keytab etl_user/$(hostname -f)@HWXDR.COM
----

** Login to beeline
+
[source,bash]
----
beeline -u "jdbc:hive2://localhost:10000/default;principal=hive/$(hostname -f)@HWXDR.COM"
----
+
image::lab1_hive_login.png[]

** Enter a query in HiveQL
+
[source,sql]
----
SHOW DATABASES;
----
+
image::lab1_hive_showdatabases.png[]

** Create table
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS students (
name VARCHAR(64)
, age INT
, gpa DECIMAL(3,2));
----
+
This should give you an error similar to the below.
+
image::lab1_hive_permissionfailure.png[]

** This indicates a Ranger Access Permission failure. The user `etl_user` does not have permission to write to the `/warehouse/tablespace/managed/hive` directory.

** To add this permission 
*** Go to the Ranger UI at: `http://hdp31.cloudera.com:6080` and navigate to the Audit tab. In the search bar enter the Start Date and User. 
+
image::lab1_hive_rangeraccessdenied.png[]
+
Confirm that its a Ranger Access Permission failure.
*** Goto the Access Manager and in the HDFS service section , click on `hdp-dr_hadoop`
+
image::lab1_hive_hdfspermissions_1.png[]

*** Click on Add New Policy and create a new policy as shown below:
+
image::lab1_hive_hdfspermissions_2.png[]
+
Save the policy and now exit the Ranger UI.

** Go back to beeline terminal and now rerun the `CREATE TABLE` command. This time it should succeed.
+
NOTE: You might have to re-login to beeline to allow the Ranger policy to take affect. 

** Insert data and describe table.
+
[source,sql]
----
INSERT INTO TABLE default.students VALUES 
('fred flintstone', 35, 1.28)
, ('barney rubble', 32, 2.32);

DESCRIBE EXTENDED default.students;
----

* Hive ACID
** Create a full CRUD transactional table.
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS acidtbl (
a INT
, b STRING);
----
** Show the defaults on the table : 
+
[source,sql]
----
SHOW CREATE TABLE acidtbl;
----
image:lab1_hive_acidtable.png[]
This provides information about the defaults: transactional(ACID) and the ORC data storage format.

** CRUD operations
*** Insert 
+
[source,sql]
----
INSERT INTO acidtbl (a,b) 
VALUES (100, "oranges")
, (200, "apples")
, (300, "bananas");
----
This operation generates a directory and file, `delta_00001_00001/bucket_0000` in HDFS. 
image:lab1_hive_acidoperations_1.png[]

*** Delete
+
[source,sql]
----
DELETE FROM acidtbl where a = 200;
----
This operation generates a directory and file, `delete_delta_00002_00002/bucket_0000`.
image:lab1_hive_acidoperations_2.png[]

*** Update
+
[source,sql]
----
UPDATE acidtbl SET b = "pears" where a = 300;
----

** _ACID in CRUD Tables_
. Hive runs on top of an append-only file system, which means Hive does not perform in-place updates or deletions. Isolation of readers and writers cannot occur in the presence of in-place updates or deletions. In this situation, a lock manager or some other mechanism, is required for isolation. These mechanisms create a problem for long-running queries.
. Instead of in-place updates, Hive decorates every row with a row ID. The row ID is a struct that consists of the following information:
The write ID that maps to the transaction that created the row
The bucket ID, a bit-backed integer with several bits of information, of the physical writer that created the row
The row ID, which numbers rows as they were written to a data file
+
image:lab1_hive_implementation_crud.png[]
. Instead of in-place deletions, Hive appends changes to the table when a deletion occurs. The deleted data becomes unavailable and the compaction process takes care of the garbage collection later.

* Practice
** Create a text file (use your favorite editor) named students.csv in /tmp that contains the following lines.
+
[source,csv]
----
1,jane,doe,senior,mathematics
2,john,smith,junior,engineering
----
+
** Load this file onto hdfs
+
[source,bash]
----
sudo su - etl_user
kinit -Vkt /etc/security/keytabs/etl_user.keytab etl_user/$(hostname -f)@HWXDR.COM
hdfs dfs -mkdir /tmp/data
hdfs dfs -chmod 777 /tmp/data
hdfs dfs -put /tmp/students.csv /tmp/data/.
hdfs dfs -chmod 777 /tmp/data/students.csv
hdfs dfs -ls /tmp/data/students.csv
----
+ 
TIP: Check the file students.csv is visible via Ambari > Files View.
+
** In beeline, create an external table schema definition that specifies the text format, loads data from students.csv located in /tmp/data.
+
[source,sql]
----
CREATE EXTERNAL TABLE IF NOT EXISTS names_text (
studentId INT
, firstName STRING
, lastName STRING
, year STRING
, major STRING)
COMMENT 'Student Names'  
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/tmp/data';
----
** Verify table exists
+
[source,sql]
----
SELECT * FROM names_text;
----
** Create the schema for a managed table.
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS names (
studentId INT
, firstName STRING
, lastName STRING
, year STRING
, major STRING)
COMMENT 'Student Names';
----
** Move External table data to Managed Table.
+
[source,sql]
----
INSERT OVERWRITE TABLE names SELECT * FROM names_text;
----
** Verify that the data from the external table resides in the managed table, and drop the external table, and verify that the data still resides in the managed table.
+
[source,sql]
----
SELECT * FROM names; 
DROP TABLE names_text;
SELECT * FROM names; 
----

* _External vs Managed Table_
. A major difference between an external and a managed (internal) table, the persistence of table data on the file system after a `DROP TABLE` statement.
.. External table drop: Hive drops only the metadata, which consists mainly of the schema definition.
.. Managed table drop: Hive deletes the data and the metadata stored in the Hive warehouse.
. You can make the external table data available after dropping it by issuing another CREATE EXTERNAL TABLE statement to load the data from the file system.

* Keys and Constraints
+
These are standard RDBMS features implemented within Hive now primarily used in warehousing use-cases.

** Use built-in surrogate key generator
+
[source,sql]
----
SELECT ROW_NUMBER() OVER () as 
row_num
, *
FROM 
students;
----
** Not Null and Constraints
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS persons (
id INT NOT NULL
,name STRING NOT NULL
,age INT);
----
** Default value
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS persons (
id INT NOT NULL
,name STRING NOT NULL
,age INT
,created_by STRING DEFAULT CURRENT_USER()
,created_date DATE DEFAULT CURRENT_DATE());
----

* Questions
. After dropping the external table names_text, what happens to the actual data in hdfs? 
. After dropping the managed table names, what happens to the actual data in hdfs? 
. How do you remove the table from the Hive Metastore _and_ remove the data stored externally? hint: `external.table.purge`
. How do you kill a query in Hive?
. Whats the `LOAD DATA ..` command in Hive? How is this different from `INSERT INTO`?

=== Analysis of Data At Rest
In this lab, we are going to import data at Rest into HDFS and analyse it with Hive.

* Data to be used for analysis is present at `/home/etl_user/datasets`. The directory contains the following:
** Taxi lookup data (taxi+_zone_lookup.csv)
** Trip data (yellow_tripdata_2019-*.csv)
** Data dictionary for the trip records
+
NOTE: The format of the lookup data is different from the trip data. Ensure this is reflected when uploading to HDFS.

* Import the trip data into HDFS (Follow the steps used above for `students.csv`,  ensure permissions are granted on HDFS properly) and create a Hive table. Hive Table sql follows:
+
[source,sql]
----
CREATE EXTERNAL TABLE IF NOT EXISTS yellowTripData (
vendorId INT
, tpepPickupDatetime TIMESTAMP
, tpepDropoffDatetime TIMESTAMP
, passengerCount INT
, tripDistance DECIMAL
, ratecodeId INT
, storeAndFwdFlag VARCHAR(1)
, puLocationId INT
, doLocationId INT
, paymentType INT
, fareAmount DECIMAL
, extra DECIMAL
, mtaTax DECIMAL
, tipAmount DECIMAL
, tollsAmount DECIMAL
, improvementSurcharge DECIMAL
, totalAmount DECIMAL
, congestionSurcharge DECIMAL)
COMMENT 'Yellow Taxi TripData'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = ",",
    "quoteChar"     = "\""
)       
LOCATION '/tmp/data/nycTaxi/'
TBLPROPERTIES 
(
    "skip.header.line.count"="1","transactional"="false"
);
----
+
IMPORTANT: https://cwiki.apache.org/confluence/display/Hive/CSV+Serde[OpenCSVSerDe] treats all columns to be of type String. Even if you create a table with non-string column types using this SerDe, the DESCRIBE TABLE output would show string column type. The type information is retrieved from the SerDe. To convert columns to the desired type in a table, you can create a view over the table that does the CAST to the desired type. Learn more about https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HiveSerDe[SerDe's] and their uses.

* Upload the lookup data to HDFS and create a lookup table.
+
[source,sql]
----
CREATE EXTERNAL TABLE IF NOT EXISTS taxiZoneLookup (
locationId INT
, borough STRING
, zone STRING
, serviceZone STRING)
COMMENT 'Taxi Zone Lookup'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = ",",
    "quoteChar"     = "\""
)       
LOCATION '/tmp/data/nycTaxi/lookup'
TBLPROPERTIES ("skip.header.line.count"="1");
----

* Find the region that has the most number of taxi pickups.
+
[source,sql]
----
SELECT 
puLocationId AS locationId
, count(*) AS cnt 
FROM yellowTripData y 
GROUP BY y.puLocationId
ORDER BY cnt;
----
+
NOTE: Did the `group by` query fail ? If yes, logout and log back into beeline with a different port

* Login to beeline using a different port
+
[source,sql]
----
beeline -u "jdbc:hive2://localhost:10500/default;principal=hive/$(hostname -f)@HWXDR.COM"
----
+
Now re-run the `group by` query from before.
+
This should give you a resultset with 263 rows with 237 locationId as the region with the most number of pickups.
image:lab1_hive_region_237.png[]

* Questions
. Why did the query succeed now?
. Whats the port number 10500 in Hive pointing to?
. Notice, the change in performance when you re-run the same query?
+
TIP: Port 10500 points to a Hive instance using Hive LLAP and that instance uses a cache. Take a quick look at the slide deck again to refresh.

* Find the zone and borough with the highest number of pickups.
+
[source,sql]
----
CREATE TEMPORARY TABLE IF NOT EXISTS tripCounts AS 
SELECT puLocationId AS locationId
, count(*) AS cnt 
FROM yellowTripData y 
GROUP BY y.puLocationId ORDER BY cnt;

SELECT x.locationId AS locationId
, x.zone AS zone
, x.borough AS borough
, x.serviceZone as serviceZone
FROM taxiZoneLookup x 
WHERE locationId=(SELECT locationId FROM tripCounts WHERE cnt=(SELECT max(cnt) FROM tripCounts));
----
If everything is correct, then you should get a resultset like the below.
+
[source]
---- 
locationid:237 ; zone: Upper East Side South ; borough: Manhattan ; servicezone : YellowZone
----
image:lab1_hive_zoneborough_pickups.png[]

* Find the peak-hours for taxi pickup. This time we use Materialized Views instead of a temporary table.
+
[source,sql]
----
CREATE MATERIALIZED VIEW IF NOT EXISTS peakHours
DISABLE REWRITE
AS
SELECT 
y.tpepPickupDateTime AS puHour
,count(*) AS puCount
FROM yellowTripData y
WHERE y.puLocationId is NOT NULL 
GROUP BY tpepPickupDateTime;

SELECT x.puHour
, x.puCount 
FROM peakHours x 
WHERE x.puCount=(SELECT max(y.puCount) FROM peakHours y);
----
And the result should look like this 
image:lab1_hive_peakhours_result.png[]

** For more details on https://cwiki.apache.org/confluence/display/Hive/Materialized+views[Materialized Views]


* Questions
. Why did we put the lookup files in a separate directory in HDFS?
. Find the region that has the most number of taxi drop-offs.
. How would you find the top-3 pick up and drop-off regions?
. What happens if a _normal_ table is created with the same name as a temporary table?
. Create a view on top of the trip data table casting the data into the desired datatypes.
. How do you enable automatic query re-writing in Materialized Views? This is a *must-do*
