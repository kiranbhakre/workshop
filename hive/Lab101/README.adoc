= Hive Workshop - Lab101
<venky@cloudera.com>
v0.1, 2019-08-16: draft
:page-layout: docs
:description: Hive Workshop Lab 101
:icons: font
:uri-fontawesome: https://fontawesome.com/v4.7.0/
:imagesdir: ./images
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc:
:toc-placement!:

[abstract]

toc::[]

=== Intro
In this lab, we will learn how to interact with Hive using HiveQL, do basic CRUD operations and work with External Tables.

* Sanity Check
** Login to Ambari and check if services are green.
+
[source]
----
username: admin
password: BadPass#1
----

* Working With HiveQL
** ssh into your cluster https://github.com/vsellappa/workshop/tree/master/connect[how-to]

** Change into the service `etl_user`.
+
[source,bash]
----
sudo su - etl_user
----
** `kinit` the user `etl_user`.
+
[source, bash]
----
kinit -Vkt /etc/security/keytabs/etl_user.keytab etl_user/$(hostname -f)@HWXDR.COM
----

** Login to beeline
+
[source,bash]
----
beeline -u "jdbc:hive2://localhost:10000/default;principal=hive/$(hostname -f)@HWXDR.COM"
----
+
image::lab1_hive_login.png[]

** Enter a query in HiveQL
+
[source,sql]
----
SHOW DATABASES;
----
+
image::lab1_hive_showdatabases.png[]

** Create table
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS students (
name VARCHAR(64)
, age INT
, gpa DECIMAL(3,2));
----
+
This should give you an error similar to the below.
+
image::lab1_hive_permissionfailure.png[]

** This indicates a Ranger Access Permission failure. The user `etl_user` does not have permission to write to the `/warehouse/tablespace/managed/hive` directory.

** To add this permission 
*** Go to the Ranger UI at: `http://hdp31.cloudera.com:6080` and navigate to the Audit tab. In the search bar enter the Start Date and User. 
+
image::lab1_hive_rangeraccessdenied.png[]
+
Confirm that its a Ranger Access Permission failure.
*** Goto the Access Manager and in the HDFS service section , click on `hdp-dr_hadoop`
+
image::lab1_hive_hdfspermissions_1.png[]

*** Click on Add New Policy and create a new policy as shown below:
+
image::lab1_hive_hdfspermissions_2.png[]
+
Save the policy and now exit the Ranger UI.

** Go back to beeline terminal and now rerun the `CREATE TABLE` command. This time it should succeed.

** Insert data and describe table.
+
[source,sql]
----
INSERT INTO TABLE default.students VALUES 
('fred flintstone', 35, 1.28)
, ('barney rubble', 32, 2.32);

DESCRIBE EXTENDED default.students;
----

* Practice
** Create a text file named students.csv in /tmp that contains the following lines.
+
[source,csv]
----
1,jane,doe,senior,mathematics
2,john,smith,junior,engineering
----
+
** Load this file onto hdfs
+
[source,bash]
----
sudo su - etl_user
kinit -Vkt /etc/security/keytabs/etl_user.keytab etl_user/$(hostname -f)@HWXDR.COM
hdfs dfs -mkdir /tmp/data
hdfs dfs -chmod 777 /tmp/data
hdfs dfs -put /tmp/students.csv /tmp/data/.
hdfs dfs -chmod 777 /tmp/data/students.csv
hdfs dfs -ls /tmp/data/students.csv
----
+ 
TIP: Check the file students.csv is visible via Ambari > Files View.
+
** In beeline, create an external table schema definition that specifies the text format, loads data from students.csv located in /tmp/data.
+
[source,sql]
----
CREATE EXTERNAL TABLE IF NOT EXISTS names_text (
studentId INT
, firstName STRING
, lastName STRING
, year STRING
, major STRING)
COMMENT 'Student Names'  
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/tmp/data';
----
** Verify table exists
+
[source,sql]
----
SELECT * FROM names_text;
----
** Create the schema for a managed table.
+
[source,sql]
----
CREATE TABLE IF NOT EXISTS names (
studentId INT
, firstName STRING
, lastName STRING
, year STRING
, major STRING)
COMMENT 'Student Names';
----
** Move External table data to Managed Table.
+
[source,sql]
----
INSERT OVERWRITE TABLE names SELECT * FROM names_text;
----
** Verify that the data from the external table resides in the managed table, and drop the external table, and verify that the data still resides in the managed table.
+
[source,sql]
----
SELECT * FROM names; 
DROP TABLE names_text;
SELECT * FROM names; 
----

* External vs Managed Table
. A major difference between an external and a managed (internal) table, the persistence of table data on the file system after a `DROP TABLE` statement.
.. External table drop: Hive drops only the metadata, which consists mainly of the schema definition.
.. Managed table drop: Hive deletes the data and the metadata stored in the Hive warehouse.
. You can make the external table data available after dropping it by issuing another CREATE EXTERNAL TABLE statement to load the data from the file system.

* Questions
. After dropping the external table names_text, what happens to the actual data in hdfs? 
. After dropping the managed table names, what happens to the actual data in hdfs? 
. How do you remove the table from the Hive Metastore _and_ remove the data stored externally? hint: `external.table.purge`
. How do you kill a query in Hive?
. Whats the `LOAD DATA ..` command in Hive? How is this different from `INSERT INTO`?

=== Analysis of Data At Rest - Part 1
In this lab, we are going to import data at Rest into HDFS and analyse it with Hive.

* Data to be used for analysis is present at `/home/etl_user/datasets/nycTaxi`. The directory contains the following:
** Taxi lookup data (taxi+_zone_lookup.csv)
** Trip data (yellow_tripdata_2019-*.csv)
** Data dictionary for the trip records
+
NOTE: The format of the lookup data is different from the trip data. Ensure this is reflected when uploading to HDFS.

* Import the trip data into HDFS and create a Hive table. Hive Table sql follows:
+
[source,sql]
----
CREATE EXTERNAL TABLE IF NOT EXISTS yellowTripData (
vendorId INT
, tpepPickupDatetime TIMESTAMP
, tpepDropoffDatetime TIMESTAMP
, passengerCount INT
, tripDistance DECIMAL
, ratecodeId INT
, storeAndFwdFlag VARCHAR(1)
, puLocationId INT
, doLocationId INT
, paymentType INT
, fareAmount DECIMAL
, extra DECIMAL
, mtaTax DECIMAL
, tipAmount DECIMAL
, tollsAmount DECIMAL
, improvementSurcharge DECIMAL
, totalAmount DECIMAL
, congestionSurcharge DECIMAL)
COMMENT 'Yellow Taxi TripData'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = ",",
    "quoteChar"     = "\""
)       
LOCATION '/tmp/data/nycTaxi/'
TBLPROPERTIES 
(
    "skip.header.line.count"="1","transactional"="false"
);
----
+
IMPORTANT: https://cwiki.apache.org/confluence/display/Hive/CSV+Serde[OpenCSVSerDe] treats all columns to be of type String. Even if you create a table with non-string column types using this SerDe, the DESCRIBE TABLE output would show string column type. The type information is retrieved from the SerDe. To convert columns to the desired type in a table, you can create a view over the table that does the CAST to the desired type. Learn more about https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-HiveSerDe[SerDe's] and their uses.

* Upload the lookup data to HDFS and create a lookup table.
+
[source,sql]
----
CREATE EXTERNAL TABLE IF NOT EXISTS taxiZoneLookup (
locationId INT
, borough STRING
, zone STRING
, serviceZone STRING)
COMMENT 'Taxi Zone Lookup'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = ",",
    "quoteChar"     = "\""
)       
LOCATION '/tmp/data/nycTaxi/lookup'
TBLPROPERTIES ("skip.header.line.count"="1");
----

* Find the region that has the most number of taxi pickups.
+
[source,sql]
----
SELECT 
puLocationId AS locationId
, count(*) AS cnt 
FROM yellowTripData y 
GROUP BY y.puLocationId
ORDER BY cnt;
----

* Find the zone and borough with the highest number of pickups.
+
[source,sql]
----
CREATE TEMPORARY TABLE IF NOT EXISTS tripCounts AS 
SELECT puLocationId AS locationId
, count(*) AS cnt 
FROM yellowTripData y 
GROUP BY y.puLocationId ORDER BY cnt;

SELECT x.locationId AS locationId
, x.zone AS zone
, x.borough AS borough
, x.serviceZone as serviceZone
FROM taxiZoneLookup x 
WHERE locationId=(SELECT locationId FROM tripCounts WHERE cnt=(SELECT max(cnt) FROM tripCounts));
----

* Questions
. Why did we put the lookup files in a separate directory in HDFS?
. Find the region that has the most number of taxi drop-offs.
. How would you find the top-3 pick up and drop-off regions?
. What happens if a _normal_ table is created with the same name as a temporary table?
. Create a view on top of the trip data table casting the data into the desired datatypes.

=== Analysis of Data At Rest - Part 2
In this lab, we will use Materialized views and transactional tables to further analyse the data.

* Find the peak-hours for taxi pickup. This time we use Materialized Views instead of a temporary table.
+
[source,sql]
----
CREATE MATERIALIZED VIEW IF NOT EXISTS peakHours
DISABLE REWRITE
AS
SELECT 
y.tpepPickupDateTime AS puHour
,count(*) AS puCount
FROM yellowTripData y
WHERE y.puLocationId is NOT NULL 
GROUP BY tpepPickupDateTime;

SELECT x.puHour
, x.puCount 
FROM peakHours x 
WHERE x.puCount=(SELECT max(y.puCount) FROM peakHours y))
----
* https://cwiki.apache.org/confluence/display/Hive/Materialized+views[Materialized Views]

* Questions
. How do you enable automatic query re-writing in Material Views? This is a *must-do*
